{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using of partial_fit()\n",
    "\n",
    "Our Goal is to find the model that can incorporate new training set data without retraining the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGDClassifier had partial_fit() function. \n",
    "# SGDClassifier can work as Logistic Regression and as SVM - it depends on \"loss\" parametr.\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training data:\n",
    "df_work = pd.read_csv('labels_and_features_20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend that initially we have only a part of our dataset, and then we get some additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data = df_work.iloc[ : 1700]           # 1700 samples\n",
    "additional_data = df_work.iloc[1700: 1800]  # 100 samples\n",
    "test_data = df_work.iloc[1800: ]            # 197 smaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = ['q_Imax', 'Imax_over_Imean', 'Imax_over_Imean_local',\n",
    "       'fluctuation_strength', 'low_q_ratio', 'high_q_ratio',\n",
    "       'Imax_over_Ilowq', 'Imax_over_Ihighq', 'Ilowq_over_Ihighq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model after initial training: [[ -3.94161337 -11.0369695   -0.63127216   4.37763106  -8.30183662\n",
      "    8.30183662   7.29089711   2.24619321  -4.13462304]]\n",
      "Accuracy on testing set\n",
      "0.989847715736\n",
      "\n",
      "\n",
      "The model after additional training: [[ -3.71695557 -11.08426576  -1.57611988   3.69125606  -8.52898932\n",
      "    8.52898932   5.51380811   3.03528943  -2.92655993]]\n",
      "Accuracy on testing set\n",
      "0.994923857868\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(init_data[fs])\n",
    "logreg = SGDClassifier(loss= 'log', random_state = 101)\n",
    "# train the model on the initial data\n",
    "logreg.fit(scaler.transform(init_data[fs]), init_data['bad_data'])\n",
    "print('The model after initial training:', logreg.coef_ )\n",
    "scores = logreg.score(scaler.transform(test_data[fs]), test_data['bad_data'])\n",
    "print('Accuracy on testing set')\n",
    "print(scores)\n",
    "print('\\n')\n",
    "\n",
    "# to save the model\n",
    "with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(logreg, fid)  \n",
    "\n",
    "# load the model\n",
    "with open('my_dumped_classifier.pkl', 'rb') as fid:\n",
    "    logreg2 = pickle.load(fid)\n",
    "\n",
    "# additional training with paritial_fit() ###################################################################\n",
    "logreg2.partial_fit(scaler.transform(additional_data[fs]), additional_data['bad_data'], classes=[True, False])\n",
    "#############################################################################################################\n",
    "\n",
    "print('The model after additional training:', logreg2.coef_ )\n",
    "scores = logreg2.score(scaler.transform(test_data[fs]), test_data['bad_data'])\n",
    "print('Accuracy on testing set')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
